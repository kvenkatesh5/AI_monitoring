{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**`SPC` Experiments for Input CXR  Monitoring**\n"
      ],
      "metadata": {
        "id": "jiD51X8ZykzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "QyU3dSW4y0EY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_utWAz7hyV3r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib import cm\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import seaborn as sns\n",
        "\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.linalg import pinv\n",
        "from scipy.spatial import distance\n",
        "\n",
        "import joblib\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "\n",
        "# Custom modules\n",
        "from feature_loader import load_and_divide_features\n",
        "from similarity_computation import compute_similarity\n",
        "import CUSUM_detector\n",
        "\n",
        "\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('darkgrid')\n",
        "np.random.seed(2023)\n",
        "random.seed(2023)"
      ],
      "metadata": {
        "id": "XmbkgLbby7HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Computing feature similarities**\n",
        "Note: Analysis is done for best feature extraction method and metrics. Here, we identifed this to be supervised VGG16 features scored by cosine similarity."
      ],
      "metadata": {
        "id": "VtL1Enb6zUm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's first load the features"
      ],
      "metadata": {
        "id": "gSdj2lsQzeLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace these paths with your desired save locations\n",
        "training_features_path = '_supervised_training_features.npy' # features of train set\n",
        "in_dist_features_path = '_supervised_in_dist_features.npy' # pool of in-dist (CXR) images\n",
        "out_dist_features_path = '_supervised_out_dist_features.npy' # pool of out-dist (non CXR) images; this can be pedatric dataset.\n",
        "\n",
        "# Load the features\n",
        "training_features = np.load(training_features_path)\n",
        "in_dist_features = np.load(in_dist_features_path)\n",
        "out_dist_features = np.load(out_dist_features_path)\n",
        "\n",
        "print(\"Features load successfully.\")"
      ],
      "metadata": {
        "id": "ZBwhcS3Wygb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize the features"
      ],
      "metadata": {
        "id": "wjSzDmuV0Fn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to perform PCA and plot\n",
        "def plot_pca(features, title, ax):\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(features)\n",
        "    ax.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('PCA Component 1')\n",
        "    ax.set_ylabel('PCA Component 2')\n",
        "\n",
        "# Create a figure with 3 subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot each dataset\n",
        "plot_pca(training_features, 'Training Features', axes[0])\n",
        "plot_pca(in_dist_features, 'In-Dist Features', axes[1])\n",
        "plot_pca(out_dist_features, 'Out-Dist Features', axes[2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O9LYY_KD0Imj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOD Metric Computation\n",
        "Compute the cosine similarity or mahalanobis distance for the features based on the functions below"
      ],
      "metadata": {
        "id": "vLT_8pxJ0eWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "from numpy.linalg import pinv\n",
        "\n",
        "def compute_similarity(tr_features, tt_features, similarity_type='cosine'):\n",
        "    \"\"\"\n",
        "    Compute similarities between training and testing features based on the specified type.\n",
        "\n",
        "    Parameters:\n",
        "    - tr_features (array): Training feature vectors.\n",
        "    - tt_features (array): Testing feature vectors.\n",
        "    - similarity_type (str): Type of similarity to compute ('cosine' or 'mahalanobis').\n",
        "\n",
        "    Returns:\n",
        "    - dict: Contains computed similarities and basic statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_cosine_similarity(tr_features_, tt_features_):\n",
        "        centroid = np.mean(tr_features_, axis=0)\n",
        "        return [1 - distance.cosine(feature, centroid) for feature in tt_features_]\n",
        "\n",
        "    def compute_mahalanobis_similarity(tr_features_, tt_features_):\n",
        "        covariance_matrix = np.cov(tr_features_, rowvar=False)\n",
        "        covariance_matrix_inv = pinv(covariance_matrix)\n",
        "        centroid = np.mean(tr_features_, axis=0)\n",
        "        return [distance.mahalanobis(feature, centroid, covariance_matrix_inv) for feature in tt_features_]\n",
        "\n",
        "    # Compute similarities\n",
        "    if similarity_type == 'cosine':\n",
        "        similarities = compute_cosine_similarity(tr_features, tt_features)\n",
        "    elif similarity_type == 'mahalanobis':\n",
        "        similarities = compute_mahalanobis_similarity(tr_features, tt_features)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid similarity type. Choose 'cosine' or 'mahalanobis'.\")\n",
        "\n",
        "    # Compute statistics\n",
        "    mean_similarity = np.mean(similarities)\n",
        "    std_similarity = np.std(similarities)\n",
        "    median_similarity = np.median(similarities)\n",
        "    mad_similarity = np.median(np.abs(similarities - median_similarity))\n",
        "    percentile_95 = np.percentile(similarities, 95)\n",
        "    percentile_99 = np.percentile(similarities, 99)\n",
        "    range_similarity = np.ptp(similarities)\n",
        "    iqr_similarity = np.percentile(similarities, 75) - np.percentile(similarities, 25)\n",
        "\n",
        "    return {\n",
        "        'similarities': similarities,\n",
        "        'mean': mean_similarity,\n",
        "        'std': std_similarity,\n",
        "        'median': median_similarity,\n",
        "        'mad': mad_similarity,\n",
        "        'percentile_95': percentile_95,\n",
        "        'percentile_99': percentile_99,\n",
        "        'range': range_similarity,\n",
        "        'iqr': iqr_similarity\n",
        "    }"
      ],
      "metadata": {
        "id": "SdSU3VNj0e2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: Cosine OOD\n",
        "# Assuming train_features, test_in_dist_features, and test_out_dist_features are defined\n",
        "cosine_train_similarities_VGG16 = compute_similarity(training_features, training_features, \"cosine\")\n",
        "cosine_train_similarities_VGG16_values = cosine_train_similarities_VGG16['similarities']\n",
        "cosine_mean_train_VGG16 = cosine_train_similarities_VGG16[\"mean\"]\n",
        "cosine_std_train_VGG16 = cosine_train_similarities_VGG16[\"std\"]\n",
        "\n",
        "\n",
        "# Computing the 3-sigma upper and lower control limits\n",
        "cosine_3UCL_train_upper_VGG16 = cosine_mean_train_VGG16 + 3 * cosine_std_train_VGG16\n",
        "cosine_3LCL_train_lower_VGG16 = cosine_mean_train_VGG16 - 3 * cosine_std_train_VGG16\n",
        "\n",
        "# In-dist pool OOD metric\n",
        "cosine_in_dist_similarities_VGG16 = compute_similarity(training_features, in_dist_features, \"cosine\")\n",
        "cosine_in_dist_similarities_VGG16_values = cosine_in_dist_similarities_VGG16['similarities']\n",
        "cosine_in_dist_similarities_VGG16_mean = cosine_in_dist_similarities_VGG16[\"mean\"]\n",
        "cosine_in_dist_similarities_VGG16_std = cosine_in_dist_similarities_VGG16[\"std\"]\n",
        "\n",
        "# Out-dist pool OOD metric\n",
        "cosine_out_dist_similarities_VGG16 = compute_similarity(training_features, out_dist_features, \"cosine\")\n",
        "cosine_out_dist_similarities_VGG16_values = cosine_out_dist_similarities_VGG16['similarities']\n",
        "cosine_out_dist_similarities_VGG16_mean = cosine_out_dist_similarities_VGG16[\"mean\"]\n",
        "cosine_out_dist_similarities_VGG16_std = cosine_out_dist_similarities_VGG16[\"std\"]\n",
        "\n",
        "\n",
        "print(\"Cosine - Mean train: {} | STD train: {}\".format(cosine_mean_train_VGG16, cosine_std_train_VGG16))\n",
        "print(\"Cosine - MEAN in-dist: {} | MEAN out-dist: {}\".format(cosine_in_dist_similarities_VGG16_mean, cosine_in_dist_similarities_VGG16_std))\n",
        "print(\"Cosine - STD out-dist: {} | STD out-dist: {}\".format(cosine_out_dist_similarities_VGG16_mean, cosine_out_dist_similarities_VGG16_std))\n"
      ],
      "metadata": {
        "id": "6WJ_M_lT1H4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the OOD metrics for train, in-dist pool, and out-dist pool"
      ],
      "metadata": {
        "id": "K5dBVm7211jW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create histograms\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Training Features Histogram\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(cosine_train_similarities_VGG16_values, bins=50, color='blue', alpha=0.7)\n",
        "plt.title('Train Features Cosine Similarity')\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# In-Dist Features Histogram\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(cosine_in_dist_similarities_VGG16_values, bins=50, color='green', alpha=0.7)\n",
        "plt.title('In-Dist Features Cosine Similarity')\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Out-Dist Features Histogram\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(cosine_out_dist_similarities_VGG16_values, bins=50, color='red', alpha=0.7)\n",
        "plt.title('Out-Dist Features Cosine Similarity')\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cGFQmzzb1o9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create 3-sigma SPC chart for randomly selected images\n",
        "Randomly pick 50 points from in-distribution and out-of-distribution cosine values, and plot them based on the control limits of train features"
      ],
      "metadata": {
        "id": "ad7CUoCh2KZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly pick 50 points from in-distribution and out-of-distribution cosine similarities\n",
        "in_dist_samples = np.random.choice(cosine_in_dist_similarities_VGG16_values, 50, replace=False)\n",
        "out_dist_samples = np.random.choice(cosine_out_dist_similarities_VGG16_values, 50, replace=False)\n",
        "\n",
        "# Combine and shuffle these 100 points\n",
        "combined_samples = np.concatenate((in_dist_samples, out_dist_samples))\n",
        "# Also, create a label array to track InD (0) and OOD (1) points\n",
        "labels = np.array([0]*50 + [1]*50)\n",
        "# Shuffle both arrays in unison\n",
        "shuffled_indices = np.random.permutation(np.arange(100))\n",
        "combined_samples = combined_samples[shuffled_indices]\n",
        "labels = labels[shuffled_indices]\n",
        "#np.random.shuffle(combined_samples)\n",
        "print(combined_samples)\n",
        "\n",
        "# Identify points as in or out of distribution based on control limits\n",
        "ood_predictions = [1 if val < cosine_3LCL_train_lower_VGG16 else 0 for val in combined_samples]"
      ],
      "metadata": {
        "id": "vs8hE6cu2acm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now, plot the chart (one image/point at a time)"
      ],
      "metadata": {
        "id": "UwR5dp-o25Pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the following are defined:\n",
        "# combined_samples - contains the mixed in-distribution and out-of-distribution points\n",
        "# ood_predictions - flags for points detected as out-of-distribution based on control limits\n",
        "# labels - actual labels indicating whether a point is in-distribution (0) or out-of-distribution (1)\n",
        "# cosine_3UCL_train_upper, cosine_3LCL_train_lower - control limits\n",
        "# cosine_mean_train - mean cosine similarity for the training set\n",
        "\n",
        "# Plot the SPC chart\n",
        "fig, ax = plt.subplots(figsize=(15, 6))\n",
        "ax.plot(combined_samples, marker='o', linestyle='-', color='grey', zorder=1)\n",
        "\n",
        "# Plot control limits and mean\n",
        "ax.axhline(np.clip(cosine_3UCL_train_upper_VGG16, a_min=0.0, a_max=1.0), color='k', linestyle='--', label='Control Limit (UCL & LCL)')\n",
        "ax.axhline(np.clip(cosine_3LCL_train_lower_VGG16, a_min=0.0, a_max=1.0), color='k', linestyle='--')\n",
        "ax.axhline(cosine_mean_train_VGG16, color='k', linestyle='-', label='Mean')\n",
        "\n",
        "# Highlight points detected as out-of-distribution with black stars\n",
        "for i, val in enumerate(combined_samples):\n",
        "    if ood_predictions[i] == 1:\n",
        "        ax.scatter(i, val, color='black', marker='o', s=60, zorder=2)\n",
        "\n",
        "# Circle actual out-of-distribution points with blue circles\n",
        "for i, val in enumerate(combined_samples):\n",
        "    if labels[i] == 1:\n",
        "        ax.scatter(i, val, marker='s', facecolors='none', edgecolors='blue', s=120, zorder=3)\n",
        "\n",
        "# Set chart properties\n",
        "ax.set_facecolor('white')\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Update the legend\n",
        "legend_elements = [plt.Line2D([0], [0], color='k', linestyle='--', label='Control Limit (UCL & LCL)'),\n",
        "                   plt.Line2D([0], [0], color='k', linestyle='-', label='Mean'),\n",
        "                   plt.Line2D([0], [0], marker='o', color='w', label='Detected OOD', markerfacecolor='black', markersize=10),\n",
        "                   plt.Line2D([0], [0], marker='s', color='w', label='Actual OOD', markerfacecolor='none', markeredgecolor='blue', markersize=10)]\n",
        "ax.legend(handles=legend_elements)\n",
        "\n",
        "# Set chart properties (adding the axis labels here)\n",
        "ax.set_xlabel('Image Samples')  # X-axis label\n",
        "ax.set_ylabel('OOD Metric')    # Y-axis label\n",
        "ax.set_facecolor('white')\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Fr_ObT3Q23wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, we compute the perfomance of SPC-based OOD for all points in both in-dist and out-dist pools."
      ],
      "metadata": {
        "id": "8faOjXK_3Y9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the cosine similarity scores and their corresponding labels\n",
        "combined_scores = np.concatenate([cosine_in_dist_similarities_VGG16_values, cosine_out_dist_similarities_VGG16_values])\n",
        "# Label: 1 for out-of-distribution, 0 for in-distribution\n",
        "combined_labels = np.concatenate([np.zeros(len(cosine_in_dist_similarities_VGG16_values)), np.ones(len(cosine_out_dist_similarities_VGG16_values))])\n",
        "\n",
        "# Apply the rule to flag points\n",
        "ood_predictions = np.array([1 if val < cosine_3LCL_train_lower_VGG16 else 0 for val in combined_scores])\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    sensitivity = recall_score(y_true, y_pred)  # Same as recall\n",
        "    specificity = recall_score(y_true, y_pred, pos_label=0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return sensitivity, specificity, accuracy\n",
        "\n",
        "# Compute initial metrics\n",
        "initial_sensitivity, initial_specificity, initial_accuracy = compute_metrics(combined_labels, ood_predictions)\n",
        "\n",
        "# Bootstrapping for confidence intervals\n",
        "n_iterations = 1000\n",
        "n_size = int(len(combined_labels) * 0.5)  # 50% sample size\n",
        "bootstrapped_metrics = []\n",
        "\n",
        "for _ in range(n_iterations):\n",
        "    # Prepare bootstrap sample\n",
        "    indices = resample(np.arange(len(combined_labels)), n_samples=n_size)\n",
        "    boot_labels = combined_labels[indices]\n",
        "    boot_predictions = ood_predictions[indices]\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = compute_metrics(boot_labels, boot_predictions)\n",
        "    bootstrapped_metrics.append(metrics)\n",
        "\n",
        "# Convert to numpy array for easy percentile computation\n",
        "bootstrapped_metrics = np.array(bootstrapped_metrics)\n",
        "\n",
        "# Compute 95% confidence intervals\n",
        "lower_p = 2.5\n",
        "upper_p = 97.5\n",
        "sensitivity_conf = np.percentile(bootstrapped_metrics[:, 0], [lower_p, upper_p])\n",
        "specificity_conf = np.percentile(bootstrapped_metrics[:, 1], [lower_p, upper_p])\n",
        "accuracy_conf = np.percentile(bootstrapped_metrics[:, 2], [lower_p, upper_p])\n",
        "\n",
        "print(\"Supervised features (initial_sensitivity)\", initial_sensitivity)\n",
        "print(\"Supervised features (sensitivity_conf)\", sensitivity_conf)\n",
        "\n",
        "print(\"Supervised features (initial_specificity)\", initial_specificity)\n",
        "print(\"Supervised features (specificity_conf)\", specificity_conf)\n",
        "\n",
        "print(\"Supervised features (initial_accuracy)\", initial_accuracy)\n",
        "print(\"Supervised features (accuracy_conf)\", accuracy_conf)\n",
        "\n"
      ],
      "metadata": {
        "id": "QKqEL6Ij3f80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simulated Monitoring Scenario**\n",
        "\n",
        "\"\"\"Time simulation\n",
        "Days 0-30: 0-3% OOD rate\n",
        "Days 30-60: 4-6% OOD rate\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "kaWhfGsa4Wa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of days and images per day\n",
        "total_days = 60\n",
        "images_per_day = 100"
      ],
      "metadata": {
        "id": "4vGu2lQK7BgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty list to store daily data\n",
        "all_data = []\n",
        "percentages_out_dist = []\n",
        "\n",
        "# Function to select data for each day\n",
        "def select_daily_data(in_dist_data, out_dist_data, out_dist_percent, day_data_count):\n",
        "    out_dist_count = int(day_data_count * out_dist_percent / 100)\n",
        "    in_dist_count = day_data_count - out_dist_count\n",
        "\n",
        "    # Randomly select out-of-distribution and in-distribution data\n",
        "    daily_out_dist_data = np.random.choice(out_dist_data, out_dist_count, replace=False)\n",
        "    daily_in_dist_data = np.random.choice(in_dist_data, in_dist_count, replace=False)\n",
        "\n",
        "    # Combine and return the daily data\n",
        "    return np.concatenate([daily_out_dist_data, daily_in_dist_data])\n",
        "\n",
        " # Simulate data for each day\n",
        "for day in range(1, total_days + 1):\n",
        "    if day <= 30:\n",
        "        percent_out_dist = (0, 3)\n",
        "    else:\n",
        "        percent_out_dist = (4,6)\n",
        "\n",
        "    # If range is given for percent, choose randomly\n",
        "    if type(percent_out_dist) == tuple:\n",
        "        p = np.random.uniform(low=percent_out_dist[0], high=percent_out_dist[1])\n",
        "    else:\n",
        "        p = percent_out_dist\n",
        "    percentages_out_dist.append(p)\n",
        "\n",
        "    daily_data = select_daily_data(\n",
        "        cosine_in_dist_similarities[\"similarities\"],\n",
        "        cosine_out_dist_similarities[\"similarities\"],\n",
        "        p,\n",
        "        images_per_day\n",
        "    )\n",
        "    all_data.append(daily_data)\n",
        "\n",
        "\n",
        "# Combine all daily data for CUSUM analysis\n",
        "avg_cusum_data = np.concatenate(all_data)\n",
        "daily_averages = [np.mean(day) for day in all_data]\n",
        "CUSUM_data_average_day = np.array(daily_averages)"
      ],
      "metadata": {
        "id": "YxwtR5_x7KX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Plotting day-averaged cosine distance\"\"\"\n",
        "plt.figure(figsize=(15, 6))\n",
        "sc = plt.scatter(range(1, total_days + 1), daily_averages, c=percentages_out_dist, cmap='coolwarm')\n",
        "# plt.colorbar(sc, label='Out-of-Distribution Percentage (%)')\n",
        "plt.colorbar(sc)\n",
        "# plt.title('Day-Averaged Cosine Similarity')\n",
        "# plt.xlabel('Day')\n",
        "# plt.ylabel('Average Cosine Distance')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.grid(True)\n",
        "plt.savefig(\"../figs/ood_simulation.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lk4dDyCY7hpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Three-sigma SPC chart for per-day averages\"\"\"\n",
        "# Define the control limits using the three-sigma rule\n",
        "mean = np.mean(daily_averages[:30])\n",
        "upper_control_limit = np.mean(daily_averages[:30]) + 3 * np.std(daily_averages[:30])\n",
        "lower_control_limit = np.mean(daily_averages[:30]) - 3 * np.std(daily_averages[:30])\n",
        "\n",
        "shift_start_day = 30\n",
        "\n",
        "# Plot the SPC chart\n",
        "fig, ax = plt.subplots(figsize=(15, 6))\n",
        "ax.plot(combined_in_out_samples, marker='o', linestyle='-', color='k', label='Daily Averages')  # Black color for plot\n",
        "\n",
        "\n",
        "\n",
        "# Plot control limits and mean\n",
        "ax.axhline(upper_control_limit_VGG, color='k', linestyle='--', label='Upper Control Limit (UCL)')  # Black color for UCL\n",
        "ax.axhline(lower_control_limit_VGG, color='k', linestyle='--', label='Lower Control Limit (LCL)')  # Black color for LCL\n",
        "ax.axhline(mu0_in_control, color='k', linestyle='-', label='Mean')  # Black color for mean\n",
        "plt.fill_between(range(len(daily_averages)), \\\n",
        "                    np.clip(lower_control_limit, a_min=0.0, a_max=1.0), \\\n",
        "                    np.clip(upper_control_limit, a_min=0.0, a_max=1.0), \\\n",
        "                    color='grey', alpha=0.1)\n",
        "\n",
        "# Highlight points outside of control limits with pink circle around them\n",
        "for i, val in enumerate(daily_averages):\n",
        "  if val < lower_control_limit:\n",
        "    ax.scatter(i, val, color='darkgrey', marker='*', s=150)  # Dark grey stars for out-of-control points\n",
        "    ax.scatter(i, val, facecolors='none', edgecolors='grey', marker='o', s=250)  # Grey circle around out-of-control points\n",
        "\n",
        "\n",
        "# Indicate the first shift point\n",
        "ax.axvline(x=shift_start_day, color='purple', linestyle='--', label='Induced Shift')  # Purple line for shift start\n",
        "\n",
        "ax.set_facecolor('white')  # White background\n",
        "ax.set_xlabel('Time (Day)')\n",
        "ax.set_ylabel('Average OOD Metric ')\n",
        "ax.legend()\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)  # Lighter grid lines for better visibility\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.savefig(\"../figs/batched_3sigma_ctr_cosine.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1LsYeDfb7oV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"CUSUM\"\"\"\n",
        "# Specify the control parameters and the threshold\n",
        "pre_change_days = 30  # Number of days the process is in-control\n",
        "total_days = 60  # Total number of days in the dataset\n",
        "control_limit = 4  # Multiplier for control limit\n",
        "detector = CUSUM_detector.CUSUMChangeDetector(pre_change_days, total_days)"
      ],
      "metadata": {
        "id": "kiHy6LLj9byp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ks = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
        "for k in ks:\n",
        "    detector.changeDetection(CUSUM_data_average_day, pre_change_days, total_days, control_limit, k, save_plot=False)\n"
      ],
      "metadata": {
        "id": "FN8ZWSN49irg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose best k\n",
        "print(detector.summary())\n",
        "# Save CUSUM for best k\n",
        "detector.changeDetection(CUSUM_data_average_day, pre_change_days, total_days, control_limit, 0.10, save_plot=True)"
      ],
      "metadata": {
        "id": "lXeVdz8E9ojQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hard coded CUSUM"
      ],
      "metadata": {
        "id": "fxtk30Gy9roD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select CUSUM paramters\n",
        "\n",
        "# Initialize lists to store results\n",
        "FalsePos = []\n",
        "TruePos = []\n",
        "AvgDD = []  # Average Detection Delay\n",
        "DetectionTimes = []\n",
        "\n",
        "# Define the range of k values as a fraction of in_std\n",
        "k_values = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "# Specify the control parameters and the threshold\n",
        "pre_change_days = 30  # Number of days the process is in-control\n",
        "total_days = 60  # Total number of days in the dataset\n",
        "control_limit = 4  # Multiplier for control limit\n",
        "delta = 1  # Change magnitude in terms of standard deviations\n",
        "\n",
        "# Split your data into in-control and out-of-control periods\n",
        "in_control_sp = CUSUM_data_average_day[:pre_change_days]\n",
        "out_control_sp = CUSUM_data_average_day[pre_change_days:total_days]\n",
        "\n",
        "# Compute the mean and standard deviation for in-control and out-of-control periods\n",
        "mu_in = np.mean(in_control_sp)\n",
        "mu_out = np.mean(out_control_sp)\n",
        "in_std = np.std(in_control_sp)\n",
        "\n",
        "#k = (delta * in_std) / 2\n",
        "print(len(CUSUM_data_average_day))\n",
        "print(mu_in)\n",
        "print(mu_out)"
      ],
      "metadata": {
        "id": "a895U4QG9zKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CUSUM function\n",
        "def CUSUM(x, mu0, k, h):\n",
        "    S_hi = [0]\n",
        "    S_lo = [0]\n",
        "    for i in range(len(x)):\n",
        "        S_hi.append(max(0, S_hi[i] + (x[i] - mu0 - k)))\n",
        "        S_lo.append(min(0, S_lo[i] + (x[i] - mu0 + k)))\n",
        "\n",
        "    S_hi = np.array(S_hi[1:])\n",
        "    S_lo = np.array(S_lo[1:])\n",
        "\n",
        "    signal_hi = np.where(S_hi > h)[0]\n",
        "    signal_lo = np.where(S_lo < -h)[0]\n",
        "    signal = np.unique(np.concatenate((signal_hi, signal_lo)))\n",
        "\n",
        "    return signal, S_hi, S_lo"
      ],
      "metadata": {
        "id": "XQeqmWQQ-KTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k_th in k_values:\n",
        "    k = k_th * in_std\n",
        "    h = control_limit * in_std  # h equal to 4*std\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    FalsePos = []\n",
        "    TruePos = []\n",
        "    AvgDD = []  # Average Detection Delay\n",
        "    DetectionTimes = []\n",
        "\n",
        "    # Call the CUSUM function\n",
        "    signal, S_hi, S_lo = CUSUM(CUSUM_data_average_day, mu_in, k, h)\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(15, 6))\n",
        "\n",
        "    ax.plot(S_hi, label='High Side CUSUM', color='blue')\n",
        "    ax.plot(S_lo, label='Low Side CUSUM', color='green')\n",
        "    ax.axhline(y=h, color='black', linestyle='--', linewidth=2, label='Threshold (+h)')\n",
        "    ax.axhline(y=-h, color='black', linestyle='--', linewidth=2, label='Threshold (-h)')\n",
        "    ax.scatter(signal, [S_hi[i] for i in signal], color='black', zorder=5, label='Detected Shift')\n",
        "    ax.scatter(signal, [S_lo[i] for i in signal], color='black', zorder=5)\n",
        "\n",
        "    # Adding vertical lines for expected shift points (every 30 days starting from day 30)\n",
        "    #for day in range(30, total_days+1, 50):\n",
        "    #  ax.axvline(x=day, color='purple', linestyle='--', label='Expected Shift Point' if day == 30 else \"\")\n",
        "\n",
        "\n",
        "    # Indicate the first shift point\n",
        "    ax.axvline(x=30, color='purple', linestyle='--', label='Low Shift (2-4%)')  # Purple line for shift start\n",
        "    # Indicate the second shift point\n",
        "    #ax.axvline(x=40, color='purple', linestyle='--', label='Second Shift (moderate)')  # Purple line for shift start\n",
        "    # Indicate the third shift point\n",
        "    #ax.axvline(x=70, color='purple', linestyle='--', label='Third Shift (high)')  # Purple line for shift start\n",
        "\n",
        "    #ax.set_title(f'Processing for k = {k}')\n",
        "    ax.set_facecolor('white')  # White background\n",
        "\n",
        "    ax.set_xlabel('Time (day)')\n",
        "    ax.set_ylabel('CUSUM Value')\n",
        "    ax.legend()\n",
        "    ax.grid(True, color='lightgrey')  # Black grid lines\n",
        "    plt.show()\n",
        "\n",
        "     # Calculate False Positives\n",
        "    for i in range(pre_change_days):\n",
        "        if S_hi[i] > h or S_lo[i] > -h:  # Assuming symmetry around zero for S_lo\n",
        "            FalsePos.append(i + 1)\n",
        "            DetectionTimes.append(i + 1)\n",
        "\n",
        "    # Calculate True Positives and Detection Delay\n",
        "    for i in range(pre_change_days, total_days):\n",
        "        if S_hi[i] > h or S_lo[i] > -h:\n",
        "            TruePos.append(i + 1)\n",
        "            AvgDD.append(i + 1 - pre_change_days)\n",
        "            break  # Remove this break if you want to count all true positives\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate MTBFA and False Alarm Rate for the current threshold\n",
        "    #MTBFA = np.mean(DetectionTimes) if DetectionTimes else float('inf')  # Avoid division by zero\n",
        "    #FalseAlarmRate = 1 / MTBFA if DetectionTimes else 0"
      ],
      "metadata": {
        "id": "nji8DV4--ST7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}