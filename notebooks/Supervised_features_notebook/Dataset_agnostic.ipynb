{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkPkUNceOe6j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.linalg import pinv\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import Model\n",
        "import joblib\n",
        "import numpy as np\n",
        "import os\n",
        "from numpy.linalg import pinv\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from scipy.spatial import distance\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "\n",
        "\n",
        "\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "qissa6GCWxTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/My Drive/CXR_modified/PadChest'\n",
        "\n",
        "# List of image paths\n",
        "image_paths = [os.path.join(data_dir, fname) for fname in os.listdir(data_dir) if fname.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Preprocessing function to load and process each image\n",
        "def process_image(image_path):\n",
        "    # Load image (adjust target_size if needed)\n",
        "    img = load_img(image_path, target_size=(224, 224))\n",
        "    img = img_to_array(img)\n",
        "    img /= 255.0  # Rescale pixel values\n",
        "    return img\n",
        "\n",
        "# Preprocessing images\n",
        "images = np.array([process_image(path) for path in image_paths])\n",
        "\n",
        "# Splitting the data into training and validation sets\n",
        "train_images, val_images = train_test_split(images, test_size=0.1, random_state=42)\n",
        "\n",
        "# Now, 'train_images' and 'val_images' are your training and validation datasets\n"
      ],
      "metadata": {
        "id": "snCPGhNOnXF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "0e677c3b-aa21-4083-f46b-007436dee1c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-234d9f6d9645>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Preprocessing images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Splitting the data into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-234d9f6d9645>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Preprocessing images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Splitting the data into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-234d9f6d9645>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Load image (adjust target_size if needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m255.0\u001b[0m  \u001b[0;31m# Rescale pixel values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         raise TypeError(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_images))\n",
        "print(len(val_images))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_images(images, title):\n",
        "    \"\"\" Display a sample of images \"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    for i in range(9):  # Display first 9 images\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "# Displaying some images from the training set\n",
        "display_images(train_images, \"Sample Images from Training Set\")\n",
        "\n",
        "# Displaying some images from the validation set\n",
        "display_images(val_images, \"Sample Images from Validation Set\")\n"
      ],
      "metadata": {
        "id": "rJnKscT7qRF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display random images from a directory\n",
        "def display_random_images_from_directory(directory, num_images=5):\n",
        "    image_names = [name for name in os.listdir(directory) if name.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))]\n",
        "    random_images = random.sample(image_names, min(num_images, len(image_names)))\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, image_name in enumerate(random_images):\n",
        "        img_path = os.path.join(directory, image_name)\n",
        "        img = Image.open(img_path).convert('L')\n",
        "        img = img.resize((256, 256), Image.ANTIALIAS)\n",
        "        plt.subplot(1, 5, i + 1)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(image_name[:20])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Paths to the testing image folders\n",
        "out_dist_folder = '/content/drive/MyDrive/out-dist-cxr'\n",
        "in_dist_folder = '/content/drive/MyDrive/in_dist_pool'\n",
        "\n",
        "# Display 5 random images from each testing folder\n",
        "print(\"Out-of-Distribution Images (Non-CXR):\")\n",
        "display_random_images_from_directory(out_dist_folder)\n",
        "\n",
        "print(\"In-Distribution Images (CXR):\")\n",
        "display_random_images_from_directory(in_dist_folder)"
      ],
      "metadata": {
        "id": "_EiqYhQ_qi4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the base VGG16 model\n",
        "base_model = VGG16(weights='imagenet', include_top=False)\n",
        "model = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)"
      ],
      "metadata": {
        "id": "z0d103y7qsJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_image(image_path, target_size=(224, 224)):\n",
        "    img = Image.open(image_path).convert('RGB')  # Convert to RGB since VGG16 expects 3 channels\n",
        "    img = img.resize(target_size)\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array * 255)\n",
        "    features = model.predict(img_array)\n",
        "    return features.flatten()\n",
        "\n",
        "def extract_features_from_folder(folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Folder not found: {folder_path}\")\n",
        "        return np.array([]), []\n",
        "    image_paths = [os.path.join(folder_path, fname) for fname in os.listdir(folder_path)]\n",
        "    features = []\n",
        "    filenames = []\n",
        "    for img_path in tqdm(image_paths):\n",
        "        try:\n",
        "            img_features = extract_features_from_image(img_path)\n",
        "            features.append(img_features)\n",
        "            filenames.append(os.path.basename(img_path))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "    return np.array(features), filenames\n",
        "\n",
        "\n",
        "def extract_features_from_generator(generator, model, num_samples):\n",
        "    batch_size = generator.batch_size\n",
        "    total_batches = (num_samples // batch_size) + (num_samples % batch_size > 0)\n",
        "    all_features = []\n",
        "\n",
        "    for _ in tqdm(range(total_batches)):\n",
        "        batch_images, _ = next(generator)\n",
        "        batch_features = model.predict(preprocess_input(batch_images * 255)).reshape(batch_images.shape[0], -1)\n",
        "        all_features.append(batch_features)\n",
        "\n",
        "    return np.vstack(all_features)"
      ],
      "metadata": {
        "id": "DsrqeKwBq1Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "def extract_features(samples):\n",
        "    \"\"\"Extract features from the samples using the model\"\"\"\n",
        "    features = model.predict(samples, batch_size=32, verbose=1)\n",
        "    return features\n",
        "\n",
        "# Extracting features from the training images\n",
        "train_features = extract_features(train_images)\n",
        "\n",
        "# Reshape the features as needed (depends on the next steps of your workflow)\n",
        "train_features = np.reshape(train_features, (train_features.shape[0], -1))\n"
      ],
      "metadata": {
        "id": "VZ62GJTArTIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = extract_features_from_folder(data_dir)\n",
        "train_features = np.reshape(train_features, (train_features.shape[0], -1))\n",
        "\n",
        "print(train_features.shape)\n",
        "print(len(train_features))"
      ],
      "metadata": {
        "id": "yO_jbqpSrawf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features and filenames using extract_features_from_folder\n",
        "train_features, train_filenames = extract_features_from_folder(data_dir)\n",
        "\n",
        "# Now, train_features is just the NumPy array of extracted features\n",
        "train_features = np.reshape(train_features, (train_features.shape[0], -1))\n",
        "\n",
        "print(train_features.shape)\n",
        "print(len(train_filenames))\n"
      ],
      "metadata": {
        "id": "IbLaONef_C3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Now, using extract_features_from_folder for the test images\n",
        "in_dist_features, in_dist_filenames = extract_features_from_folder(in_dist_folder)\n",
        "out_dist_features, out_dist_filenames = extract_features_from_folder(out_dist_folder)\n",
        "print(in_dist_features.shape)\n",
        "print(out_dist_features.shape)\n",
        "print(len(in_dist_features))"
      ],
      "metadata": {
        "id": "XUQHGCykrl_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(in_dist_features.shape)\n",
        "print(out_dist_features.shape)\n",
        "print(len(in_dist_features))\n",
        "print(len(out_dist_features))"
      ],
      "metadata": {
        "id": "TxlUOmqsr3ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Replace these paths with your desired save locations\n",
        "training_features_path = '/content/drive/My Drive/PadChest_supervised_training_features.npy'\n",
        "in_dist_features_path = '/content/drive/My Drive/PadChest__supervised_in_dist_features.npy'\n",
        "out_dist_features_path = '/content/drive/My Drive/PadChest_supervised_out_dist_features.npy'\n",
        "\n",
        "# Ensure train_features is a numpy array\n",
        "train_features = np.array(train_features)\n",
        "\n",
        "# Check the shape of train_features\n",
        "print(\"Shape of train_features:\", train_features.shape)\n",
        "\n",
        "# Now, try saving the features\n",
        "training_features_path = \"/content/drive/My Drive/PadChest_supervised_training_features.npy\"  # replace with your desired path\n",
        "np.save(training_features_path, train_features)\n",
        "\n",
        "# Save the features\n",
        "np.save(training_features_path, train_features)\n",
        "np.save(in_dist_features_path, in_dist_features)\n",
        "np.save(out_dist_features_path, out_dist_features)\n",
        "\n",
        "print(\"Features saved successfully.\")"
      ],
      "metadata": {
        "id": "PhvDoYcWrs1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the features\n",
        "training_features = np.load(training_features_path)\n",
        "in_dist_features = np.load(in_dist_features_path)\n",
        "out_dist_features = np.load(out_dist_features_path)"
      ],
      "metadata": {
        "id": "w4U6m3MqtH0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Function to perform PCA and plot\n",
        "def plot_pca(features, title, ax):\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(features)\n",
        "    ax.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('PCA Component 1')\n",
        "    ax.set_ylabel('PCA Component 2')\n",
        "\n",
        "# Create a figure with 3 subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot each dataset\n",
        "plot_pca(training_features, 'Training Features', axes[0])\n",
        "plot_pca(in_dist_features, 'In-Dist Features', axes[1])\n",
        "plot_pca(out_dist_features, 'Out-Dist Features', axes[2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u2yAVHwztOpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "from numpy.linalg import pinv\n",
        "\n",
        "def compute_similarity(tr_features, tt_features, similarity_type='cosine'):\n",
        "    \"\"\"\n",
        "    Compute similarities between training and testing features based on the specified type.\n",
        "\n",
        "    Parameters:\n",
        "    - tr_features (array): Training feature vectors.\n",
        "    - tt_features (array): Testing feature vectors.\n",
        "    - similarity_type (str): Type of similarity to compute ('cosine' or 'mahalanobis').\n",
        "\n",
        "    Returns:\n",
        "    - dict: Contains computed similarities and basic statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_cosine_similarity(tr_features_, tt_features_):\n",
        "        centroid = np.mean(tr_features_, axis=0)\n",
        "        return [1 - distance.cosine(feature, centroid) for feature in tt_features_]\n",
        "\n",
        "    def compute_mahalanobis_similarity(tr_features_, tt_features_):\n",
        "        covariance_matrix = np.cov(tr_features_, rowvar=False)\n",
        "        covariance_matrix_inv = pinv(covariance_matrix)\n",
        "        centroid = np.mean(tr_features_, axis=0)\n",
        "        return [distance.mahalanobis(feature, centroid, covariance_matrix_inv) for feature in tt_features_]\n",
        "\n",
        "    # Compute similarities\n",
        "    if similarity_type == 'cosine':\n",
        "        similarities = compute_cosine_similarity(tr_features, tt_features)\n",
        "    elif similarity_type == 'mahalanobis':\n",
        "        similarities = compute_mahalanobis_similarity(tr_features, tt_features)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid similarity type. Choose 'cosine' or 'mahalanobis'.\")\n",
        "\n",
        "    # Compute statistics\n",
        "    mean_similarity = np.mean(similarities)\n",
        "    std_similarity = np.std(similarities)\n",
        "    median_similarity = np.median(similarities)\n",
        "    mad_similarity = np.median(np.abs(similarities - median_similarity))\n",
        "    percentile_95 = np.percentile(similarities, 95)\n",
        "    percentile_99 = np.percentile(similarities, 99)\n",
        "    range_similarity = np.ptp(similarities)\n",
        "    iqr_similarity = np.percentile(similarities, 75) - np.percentile(similarities, 25)\n",
        "\n",
        "    return {\n",
        "        'similarities': similarities,\n",
        "        'mean': mean_similarity,\n",
        "        'std': std_similarity,\n",
        "        'median': median_similarity,\n",
        "        'mad': mad_similarity,\n",
        "        'percentile_95': percentile_95,\n",
        "        'percentile_99': percentile_99,\n",
        "        'range': range_similarity,\n",
        "        'iqr': iqr_similarity\n",
        "    }\n"
      ],
      "metadata": {
        "id": "-_Xa4YtPtbkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming train_features, test_in_dist_features, and test_out_dist_features are defined\n",
        "cosine_train_similarities_VGG16_PadChest = compute_similarity(training_features, training_features, \"cosine\")\n",
        "cosine_train_similarities_actaul_VGG16_PadChest = cosine_train_similarities_VGG16_PadChest['similarities']\n",
        "cosine_mean_train_VGG16_PadChest = cosine_train_similarities_VGG16_PadChest[\"mean\"]\n",
        "cosine_std_train_VGG16_PadChest = cosine_train_similarities_VGG16_PadChest[\"std\"]\n",
        "\n",
        "\n",
        "# Computing the 3-sigma upper and lower control limits\n",
        "cosine_3UCL_train_upper_VGG16_PadChest = cosine_mean_train_VGG16_PadChest + 3 * cosine_std_train_VGG16_PadChest\n",
        "cosine_3LCL_train_lower_VGG16_PadChest = cosine_mean_train_VGG16_PadChest - 3 * cosine_std_train_VGG16_PadChest\n",
        "\n",
        "\n",
        "\n",
        "cosine_in_dist_similarities_VGG16_PadChest = compute_similarity(training_features, in_dist_features, \"cosine\")\n",
        "cosine_in_dist_similarities_VGG16_actual_PadChest = cosine_in_dist_similarities_VGG16_PadChest['similarities']\n",
        "\n",
        "cosine_out_dist_similarities_VGG16_PadChest = compute_similarity(training_features, out_dist_features, \"cosine\")\n",
        "cosine_out_dist_similarities_VGG16_actual_PadChest = cosine_out_dist_similarities_VGG16_PadChest['similarities']\n",
        "\n",
        "cosine_in_dist_similarities_VGG16_mean_PadChest = cosine_in_dist_similarities_VGG16_PadChest[\"mean\"]\n",
        "cosine_in_dist_similarities_VGG16_std_PadChest = cosine_in_dist_similarities_VGG16_PadChest[\"std\"]\n",
        "\n",
        "cosine_out_dist_similarities_VGG16_mean_PadChest = cosine_out_dist_similarities_VGG16_PadChest[\"mean\"]\n",
        "cosine_out_dist_similarities_VGG16_std_PadChest = cosine_out_dist_similarities_VGG16_PadChest[\"std\"]\n",
        "\n",
        "print(\"Cosine - Mean train: {} | STD train: {}\".format(cosine_mean_train_VGG16_PadChest, cosine_std_train_VGG16_PadChest))\n",
        "print(\"Cosine - MEAN in-dist: {} | MEAN out-dist: {}\".format(cosine_in_dist_similarities_VGG16_mean_PadChest, cosine_in_dist_similarities_VGG16_std_PadChest))\n",
        "print(\"Cosine - STD out-dist: {} | STD out-dist: {}\".format(cosine_out_dist_similarities_VGG16_mean_PadChest, cosine_out_dist_similarities_VGG16_std_PadChest))"
      ],
      "metadata": {
        "id": "rcfgtSnvtqqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create histograms\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Training Features Histogram\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(cosine_train_similarities_actaul_VGG16_PadChest, bins=50, color='blue', alpha=0.7)\n",
        "plt.title('Train Features Cosine Similarity')\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# In-Dist Features Histogram\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(cosine_in_dist_similarities_VGG16_actual_PadChest, bins=50, color='green', alpha=0.7)\n",
        "plt.title('In-Dist Features Cosine Similarity')\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Out-Dist Features Histogram\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(cosine_out_dist_similarities_VGG16_actual_PadChest, bins=50, color='red', alpha=0.7)\n",
        "plt.title('Out-Dist Features Cosine Similarity')\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7oaeLsfbuy1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly pick 50 points from in-distribution and out-of-distribution cosine similarities\n",
        "in_dist_samples = np.random.choice(cosine_in_dist_similarities_VGG16_actual_PadChest, 50, replace=False)\n",
        "out_dist_samples = np.random.choice(cosine_out_dist_similarities_VGG16_actual_PadChest, 50, replace=False)\n",
        "\n",
        "# Combine and shuffle these 100 points\n",
        "combined_samples = np.concatenate((in_dist_samples, out_dist_samples))\n",
        "# Also, create a label array to track InD (0) and OOD (1) points\n",
        "labels = np.array([0]*50 + [1]*50)\n",
        "# Shuffle both arrays in unison\n",
        "shuffled_indices = np.random.permutation(np.arange(100))\n",
        "combined_samples = combined_samples[shuffled_indices]\n",
        "labels = labels[shuffled_indices]\n",
        "#np.random.shuffle(combined_samples)\n",
        "print(combined_samples)\n",
        "\n",
        "# Identify points as in or out of distribution based on control limits\n",
        "ood_predictions = [1 if val < cosine_3LCL_train_lower_VGG16_PadChest else 0 for val in combined_samples]"
      ],
      "metadata": {
        "id": "5ZNova8BB9VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming the following are defined:\n",
        "# combined_samples - contains the mixed in-distribution and out-of-distribution points\n",
        "# ood_predictions - flags for points detected as out-of-distribution based on control limits\n",
        "# labels - actual labels indicating whether a point is in-distribution (0) or out-of-distribution (1)\n",
        "# cosine_3UCL_train_upper, cosine_3LCL_train_lower - control limits\n",
        "# cosine_mean_train - mean cosine similarity for the training set\n",
        "\n",
        "# Plot the SPC chart\n",
        "fig, ax = plt.subplots(figsize=(15, 6))\n",
        "ax.plot(combined_samples, marker='o', linestyle='-', color='grey', zorder=1)\n",
        "\n",
        "# Plot control limits and mean\n",
        "ax.axhline(np.clip(cosine_3UCL_train_upper_VGG16_PadChest, a_min=0.0, a_max=1.0), color='k', linestyle='--', label='Control Limit (UCL & LCL)')\n",
        "ax.axhline(np.clip(cosine_3LCL_train_lower_VGG16_PadChest, a_min=0.0, a_max=1.0), color='k', linestyle='--')\n",
        "ax.axhline(cosine_mean_train_VGG16_PadChest, color='k', linestyle='-', label='Mean')\n",
        "\n",
        "# Highlight points detected as out-of-distribution with black stars\n",
        "for i, val in enumerate(combined_samples):\n",
        "    if ood_predictions[i] == 1:\n",
        "        ax.scatter(i, val, color='black', marker='o', s=60, zorder=2)\n",
        "\n",
        "# Circle actual out-of-distribution points with blue circles\n",
        "for i, val in enumerate(combined_samples):\n",
        "    if labels[i] == 1:\n",
        "        ax.scatter(i, val, marker='s', facecolors='none', edgecolors='blue', s=120, zorder=3)\n",
        "\n",
        "# Set chart properties\n",
        "ax.set_facecolor('white')\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Update the legend\n",
        "legend_elements = [plt.Line2D([0], [0], color='k', linestyle='--', label='Control Limit (UCL & LCL)'),\n",
        "                   plt.Line2D([0], [0], color='k', linestyle='-', label='Mean'),\n",
        "                   plt.Line2D([0], [0], marker='o', color='w', label='Detected OOD', markerfacecolor='black', markersize=10),\n",
        "                   plt.Line2D([0], [0], marker='s', color='w', label='Actual OOD', markerfacecolor='none', markeredgecolor='blue', markersize=10)]\n",
        "ax.legend(handles=legend_elements)\n",
        "\n",
        "# Set chart properties (adding the axis labels here)\n",
        "ax.set_xlabel('Image Samples')  # X-axis label\n",
        "ax.set_ylabel('OOD Metric')    # Y-axis label\n",
        "ax.set_facecolor('white')\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OGu89FfFCPrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Combine the cosine similarity scores and their corresponding labels\n",
        "combined_scores = np.concatenate([cosine_in_dist_similarities_VGG16_actual_PadChest, cosine_out_dist_similarities_VGG16_actual_PadChest])\n",
        "# Label: 1 for out-of-distribution, 0 for in-distribution\n",
        "combined_labels = np.concatenate([np.zeros(len(cosine_in_dist_similarities_VGG16_actual_PadChest)), np.ones(len(cosine_out_dist_similarities_VGG16_actual_PadChest))])\n",
        "\n",
        "# Apply the rule to flag points\n",
        "ood_predictions = np.array([1 if val < cosine_3LCL_train_lower_VGG16_PadChest else 0 for val in combined_scores])\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    sensitivity = recall_score(y_true, y_pred)  # Same as recall\n",
        "    specificity = recall_score(y_true, y_pred, pos_label=0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return sensitivity, specificity, accuracy\n",
        "\n",
        "# Compute initial metrics\n",
        "initial_sensitivity, initial_specificity, initial_accuracy = compute_metrics(combined_labels, ood_predictions)\n",
        "\n",
        "# Bootstrapping for confidence intervals\n",
        "n_iterations = 500\n",
        "n_size = int(len(combined_labels) * 0.5)  # 50% sample size\n",
        "bootstrapped_metrics = []\n",
        "\n",
        "for _ in range(n_iterations):\n",
        "    # Prepare bootstrap sample\n",
        "    indices = resample(np.arange(len(combined_labels)), n_samples=n_size)\n",
        "    boot_labels = combined_labels[indices]\n",
        "    boot_predictions = ood_predictions[indices]\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = compute_metrics(boot_labels, boot_predictions)\n",
        "    bootstrapped_metrics.append(metrics)\n",
        "\n",
        "# Convert to numpy array for easy percentile computation\n",
        "bootstrapped_metrics = np.array(bootstrapped_metrics)\n",
        "\n",
        "# Compute 95% confidence intervals\n",
        "lower_p = 2.5\n",
        "upper_p = 97.5\n",
        "sensitivity_conf = np.percentile(bootstrapped_metrics[:, 0], [lower_p, upper_p])\n",
        "specificity_conf = np.percentile(bootstrapped_metrics[:, 1], [lower_p, upper_p])\n",
        "accuracy_conf = np.percentile(bootstrapped_metrics[:, 2], [lower_p, upper_p])\n",
        "\n",
        "# Compute 95% confidence intervals\n",
        "lower_p = 2.5\n",
        "upper_p = 97.5\n",
        "sensitivity_conf = np.percentile(bootstrapped_metrics[:, 0], [lower_p, upper_p])\n",
        "specificity_conf = np.percentile(bootstrapped_metrics[:, 1], [lower_p, upper_p])\n",
        "accuracy_conf = np.percentile(bootstrapped_metrics[:, 2], [lower_p, upper_p])\n",
        "\n",
        "print(\"Supervised features (initial_sensitivity)\", initial_sensitivity)\n",
        "print(\"Supervised features (sensitivity_conf)\", sensitivity_conf)\n",
        "\n",
        "print(\"Supervised features (initial_specificity)\", initial_specificity)\n",
        "print(\"Supervised features (specificity_conf)\", specificity_conf)\n",
        "\n",
        "print(\"Supervised features (initial_accuracy)\", initial_accuracy)\n",
        "print(\"Supervised features (accuracy_conf)\", accuracy_conf)"
      ],
      "metadata": {
        "id": "9mjSFVCBCmxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pedatric CXR Test: **\n",
        "Now, we use the same training set but the test set will be pedatric!"
      ],
      "metadata": {
        "id": "MgpNDgzpErs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display random images from a directory\n",
        "def display_random_images_from_directory(directory, num_images=5):\n",
        "    image_names = [name for name in os.listdir(directory) if name.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))]\n",
        "    random_images = random.sample(image_names, min(num_images, len(image_names)))\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, image_name in enumerate(random_images):\n",
        "        img_path = os.path.join(directory, image_name)\n",
        "        img = Image.open(img_path).convert('L')\n",
        "        img = img.resize((256, 256), Image.ANTIALIAS)\n",
        "        plt.subplot(1, 5, i + 1)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(image_name[:20])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Paths to the testing image folders\n",
        "out_dist_folder_pedatric = '/content/drive/MyDrive/Pedatric_CXR'\n",
        "\n",
        "# Display 5 random images from each testing folder\n",
        "print(\"Out-of-Distribution Images (CXR-Pedatric):\")\n",
        "display_random_images_from_directory(out_dist_folder_pedatric)\n"
      ],
      "metadata": {
        "id": "9qeUodOpGIkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, using extract_features_from_folder for the test images\n",
        "in_dist_features, in_dist_filenames = extract_features_from_folder(in_dist_folder)\n",
        "out_dist_features, out_dist_filenames = extract_features_from_folder(out_dist_folder)\n",
        "out_dist_features_pedatric, out_dist_filenames_pedatric = extract_features_from_folder(out_dist_folder_pedatric)\n",
        "print(in_dist_features.shape)\n",
        "print(out_dist_features.shape)\n",
        "print(len(in_dist_features))"
      ],
      "metadata": {
        "id": "1e7zyaCVE5cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Function to perform PCA and plot\n",
        "def plot_pca(features, title, ax):\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(features)\n",
        "    ax.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('PCA Component 1')\n",
        "    ax.set_ylabel('PCA Component 2')\n",
        "\n",
        "# Create a figure with 3 subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot each dataset\n",
        "plot_pca(training_features, 'Training Features', axes[0])\n",
        "plot_pca(in_dist_features, 'In-Dist Features', axes[1])\n",
        "plot_pca(out_dist_features_pedatric, 'Out-Dist Pedatric Features', axes[2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QiDk3FV0ZcAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "cosine_out_dist_similarities_VGG16_Pedatric = compute_similarity(training_features, out_dist_features_pedatric, \"cosine\")\n",
        "cosine_out_dist_similarities_VGG16_actual_Pedatric = cosine_out_dist_similarities_VGG16_Pedatric['similarities']\n",
        "\n",
        "cosine_out_dist_similarities_VGG16_mean_Pedatric = cosine_out_dist_similarities_VGG16_Pedatric[\"mean\"]\n",
        "cosine_out_dist_similarities_VGG16_std_Pedatric = cosine_out_dist_similarities_VGG16_Pedatric[\"std\"]\n",
        "\n",
        "print(\"Cosine - Mean train: {} | STD train: {}\".format(cosine_mean_train_VGG16_PadChest, cosine_std_train_VGG16_PadChest))\n",
        "print(\"Cosine - MEAN in-dist: {} | MEAN out-dist: {}\".format(cosine_in_dist_similarities_VGG16_mean_PadChest, cosine_in_dist_similarities_VGG16_std_PadChest))\n",
        "print(\"Cosine - STD out-dist Pedatric: {} | STD out-dist Pedatric: {}\".format(cosine_out_dist_similarities_VGG16_mean_Pedatric, cosine_out_dist_similarities_VGG16_std_Pedatric))"
      ],
      "metadata": {
        "id": "GxLQ2fe9ZuzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create histograms\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Training Features Histogram\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(cosine_train_similarities_actaul_VGG16_PadChest, bins=50, color='blue', alpha=0.7)\n",
        "plt.title('Train Features Cosine Similarity')\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# In-Dist Features Histogram\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(cosine_in_dist_similarities_VGG16_actual_PadChest, bins=50, color='green', alpha=0.7)\n",
        "plt.title('In-Dist Features Cosine Similarity')\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Out-Dist Features Histogram\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(cosine_out_dist_similarities_VGG16_actual_Pedatric, bins=50, color='red', alpha=0.7)\n",
        "plt.title('Out-Dist Pedatric Features Cosine Similarity')\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dhlttrtOaP5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly pick 50 points from in-distribution and out-of-distribution cosine similarities\n",
        "in_dist_samples = np.random.choice(cosine_in_dist_similarities_VGG16_actual_PadChest, 50, replace=False)\n",
        "out_dist_samples_pedatric = np.random.choice(cosine_out_dist_similarities_VGG16_actual_Pedatric, 50, replace=False)\n",
        "\n",
        "# Combine and shuffle these 100 points\n",
        "combined_samples = np.concatenate((in_dist_samples, out_dist_samples_pedatric))\n",
        "# Also, create a label array to track InD (0) and OOD (1) points\n",
        "labels = np.array([0]*50 + [1]*50)\n",
        "# Shuffle both arrays in unison\n",
        "shuffled_indices = np.random.permutation(np.arange(100))\n",
        "combined_samples = combined_samples[shuffled_indices]\n",
        "labels = labels[shuffled_indices]\n",
        "#np.random.shuffle(combined_samples)\n",
        "print(combined_samples)\n",
        "\n",
        "\n",
        "# Computing the 3-sigma upper and lower control limits\n",
        "cosine_3UCL_train_upper_VGG16_PadChest_2Sigma = cosine_mean_train_VGG16_PadChest + 1 * cosine_std_train_VGG16_PadChest\n",
        "cosine_3LCL_train_lower_VGG16_PadChest__2Sigma = cosine_mean_train_VGG16_PadChest - 1 * cosine_std_train_VGG16_PadChest\n",
        "\n",
        "\n",
        "# Identify points as in or out of distribution based on control limits\n",
        "ood_predictions = [1 if val < cosine_3LCL_train_lower_VGG16_PadChest__2Sigma else 0 for val in combined_samples]"
      ],
      "metadata": {
        "id": "CUaf8SzCagvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming the following are defined:\n",
        "# combined_samples - contains the mixed in-distribution and out-of-distribution points\n",
        "# ood_predictions - flags for points detected as out-of-distribution based on control limits\n",
        "# labels - actual labels indicating whether a point is in-distribution (0) or out-of-distribution (1)\n",
        "# cosine_3UCL_train_upper, cosine_3LCL_train_lower - control limits\n",
        "# cosine_mean_train - mean cosine similarity for the training set\n",
        "\n",
        "# Plot the SPC chart\n",
        "fig, ax = plt.subplots(figsize=(15, 6))\n",
        "ax.plot(combined_samples, marker='o', linestyle='-', color='grey', zorder=1)\n",
        "\n",
        "# Plot control limits and mean\n",
        "ax.axhline(np.clip(cosine_3UCL_train_upper_VGG16_PadChest_2Sigma, a_min=0.0, a_max=1.0), color='k', linestyle='--', label='Control Limit (UCL & LCL)')\n",
        "ax.axhline(np.clip(cosine_3LCL_train_lower_VGG16_PadChest__2Sigma, a_min=0.0, a_max=1.0), color='k', linestyle='--')\n",
        "ax.axhline(cosine_mean_train_VGG16_PadChest, color='k', linestyle='-', label='Mean')\n",
        "\n",
        "# Highlight points detected as out-of-distribution with black stars\n",
        "for i, val in enumerate(combined_samples):\n",
        "    if ood_predictions[i] == 1:\n",
        "        ax.scatter(i, val, color='black', marker='o', s=60, zorder=2)\n",
        "\n",
        "# Circle actual out-of-distribution points with blue circles\n",
        "for i, val in enumerate(combined_samples):\n",
        "    if labels[i] == 1:\n",
        "        ax.scatter(i, val, marker='s', facecolors='none', edgecolors='blue', s=120, zorder=3)\n",
        "\n",
        "# Set chart properties\n",
        "ax.set_facecolor('white')\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Update the legend\n",
        "legend_elements = [plt.Line2D([0], [0], color='k', linestyle='--', label='Control Limit (UCL & LCL)'),\n",
        "                   plt.Line2D([0], [0], color='k', linestyle='-', label='Mean'),\n",
        "                   plt.Line2D([0], [0], marker='o', color='w', label='Detected OOD', markerfacecolor='black', markersize=10),\n",
        "                   plt.Line2D([0], [0], marker='s', color='w', label='Actual OOD', markerfacecolor='none', markeredgecolor='blue', markersize=10)]\n",
        "ax.legend(handles=legend_elements)\n",
        "\n",
        "# Set chart properties (adding the axis labels here)\n",
        "ax.set_xlabel('Image Samples')  # X-axis label\n",
        "ax.set_ylabel('OOD Metric')    # Y-axis label\n",
        "ax.set_facecolor('white')\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Kn9z_tpKauCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Combine the cosine similarity scores and their corresponding labels\n",
        "combined_scores = np.concatenate([cosine_in_dist_similarities_VGG16_actual_PadChest, cosine_out_dist_similarities_VGG16_actual_Pedatric])\n",
        "# Label: 1 for out-of-distribution, 0 for in-distribution\n",
        "combined_labels = np.concatenate([np.zeros(len(cosine_in_dist_similarities_VGG16_actual_PadChest)), np.ones(len(cosine_out_dist_similarities_VGG16_actual_Pedatric))])\n",
        "\n",
        "# Apply the rule to flag points\n",
        "ood_predictions = np.array([1 if val < cosine_3LCL_train_lower_VGG16_PadChest__2Sigma else 0 for val in combined_scores])\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    sensitivity = recall_score(y_true, y_pred)  # Same as recall\n",
        "    specificity = recall_score(y_true, y_pred, pos_label=0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return sensitivity, specificity, accuracy\n",
        "\n",
        "# Compute initial metrics\n",
        "initial_sensitivity, initial_specificity, initial_accuracy = compute_metrics(combined_labels, ood_predictions)\n",
        "\n",
        "# Bootstrapping for confidence intervals\n",
        "n_iterations = 500\n",
        "n_size = int(len(combined_labels) * 0.5)  # 50% sample size\n",
        "bootstrapped_metrics = []\n",
        "\n",
        "for _ in range(n_iterations):\n",
        "    # Prepare bootstrap sample\n",
        "    indices = resample(np.arange(len(combined_labels)), n_samples=n_size)\n",
        "    boot_labels = combined_labels[indices]\n",
        "    boot_predictions = ood_predictions[indices]\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = compute_metrics(boot_labels, boot_predictions)\n",
        "    bootstrapped_metrics.append(metrics)\n",
        "\n",
        "# Convert to numpy array for easy percentile computation\n",
        "bootstrapped_metrics = np.array(bootstrapped_metrics)\n",
        "\n",
        "# Compute 95% confidence intervals\n",
        "lower_p = 2.5\n",
        "upper_p = 97.5\n",
        "sensitivity_conf = np.percentile(bootstrapped_metrics[:, 0], [lower_p, upper_p])\n",
        "specificity_conf = np.percentile(bootstrapped_metrics[:, 1], [lower_p, upper_p])\n",
        "accuracy_conf = np.percentile(bootstrapped_metrics[:, 2], [lower_p, upper_p])\n",
        "\n",
        "# Compute 95% confidence intervals\n",
        "lower_p = 2.5\n",
        "upper_p = 97.5\n",
        "sensitivity_conf = np.percentile(bootstrapped_metrics[:, 0], [lower_p, upper_p])\n",
        "specificity_conf = np.percentile(bootstrapped_metrics[:, 1], [lower_p, upper_p])\n",
        "accuracy_conf = np.percentile(bootstrapped_metrics[:, 2], [lower_p, upper_p])\n",
        "\n",
        "print(\"Pedatric features (initial_sensitivity)\", initial_sensitivity)\n",
        "print(\"Pedatric features (sensitivity_conf)\", sensitivity_conf)\n",
        "\n",
        "print(\"Pedatric features (initial_specificity)\", initial_specificity)\n",
        "print(\"Pedatric features (specificity_conf)\", specificity_conf)\n",
        "\n",
        "print(\"Pedatric features (initial_accuracy)\", initial_accuracy)\n",
        "print(\"Pedatric features (accuracy_conf)\", accuracy_conf)"
      ],
      "metadata": {
        "id": "0dG_gR30btgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, I want to 60 images and use these samples to create the follwoing charts:\n",
        "# Three-sigma chart, Bernouuli CUSUM, CUSUM\n",
        "\n",
        "\n",
        "# Take 30 random samples without replacement from each\n",
        "in_dist_samples = np.random.choice(cosine_in_dist_similarities_VGG16_actual_PadChest, 30, replace=False)\n",
        "out_dist_samples_pedatric = np.random.choice(cosine_out_dist_similarities_VGG16_actual_Pedatric, 30, replace=False)\n",
        "\n",
        "# Combine the two sets of samples into one vector\n",
        "combined_in_out_samples = np.concatenate((in_dist_samples, out_dist_samples_pedatric))\n",
        "print(combined_samples)"
      ],
      "metadata": {
        "id": "dvAmy6V4dYnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shift_start_day = 30  # The point at which the shift occurs\n",
        "mu0_in_control = np.mean(combined_in_out_samples[:shift_start_day])  # Mean of the in-control data\n",
        "std0_in_control = np.std(combined_in_out_samples[:shift_start_day])  # Standard deviation of the in-control data\n",
        "print(mu0_in_control)\n",
        "print(std0_in_control)"
      ],
      "metadata": {
        "id": "Nho9fLPevRhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select CUSUM paramters\n",
        "\n",
        "# Initialize lists to store results\n",
        "FalsePos = []\n",
        "TruePos = []\n",
        "AvgDD = []  # Average Detection Delay\n",
        "DetectionTimes = []\n",
        "\n",
        "# Define the range of k values as a fraction of in_std\n",
        "k_values = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "# Specify the control parameters and the threshold\n",
        "pre_change_days = 30  # Number of days the process is in-control\n",
        "total_days = 60  # Total number of days in the dataset\n",
        "control_limit = 4  # Multiplier for control limit\n",
        "delta = 1  # Change magnitude in terms of standard deviations\n",
        "\n",
        "# Split your data into in-control and out-of-control periods\n",
        "in_control_sp = combined_in_out_samples[:pre_change_days]\n",
        "out_control_sp = combined_in_out_samples[pre_change_days:total_days]\n",
        "\n",
        "# Compute the mean and standard deviation for in-control and out-of-control periods\n",
        "mu_in = np.mean(in_control_sp)\n",
        "mu_out = np.mean(out_control_sp)\n",
        "in_std = np.std(in_control_sp)\n",
        "\n",
        "#k = (delta * in_std) / 2\n",
        "print(len(combined_in_out_samples))\n",
        "print(mu_in)\n",
        "print(mu_out)"
      ],
      "metadata": {
        "id": "pCu8K-0OyscP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function creates a daily pool of in-dist and out-dist test set based on the decided percentage of shift\n",
        "\n",
        "# Define the number of days and distances per day\n",
        "total_days = 60\n",
        "distances_per_day = 100\n",
        "\n",
        "# Create an empty list to store daily data\n",
        "all_data = []\n",
        "\n",
        "# Function to select data for each day\n",
        "def select_daily_data(in_dist_data, out_dist_data, out_dist_percent, day_data_count):\n",
        "    out_dist_count = int(day_data_count * out_dist_percent / 100)\n",
        "    in_dist_count = day_data_count - out_dist_count\n",
        "\n",
        "    # Randomly select out-of-distribution and in-distribution data\n",
        "    daily_out_dist_data = np.random.choice(out_dist_data, out_dist_count, replace=False)\n",
        "    daily_in_dist_data = np.random.choice(in_dist_data, in_dist_count, replace=False)\n",
        "\n",
        "    # Combine and return the daily data\n",
        "    return np.concatenate([daily_out_dist_data, daily_in_dist_data])\n",
        "\n",
        " # Simulate data for each day. This case is a small shift\n",
        "for day in range(1, total_days + 1):\n",
        "    if day <= 30:\n",
        "        percent_out_dist = 2  # First month: pre shift\n",
        "    elif day <= 60:\n",
        "        percent_out_dist = 4  # First Shift: 4% OOD\n",
        "    # elif day <= 70:\n",
        "    #    percent_out_dist = 7 # Second Shift: 7% OOD\n",
        "    # elif day <= 100:\n",
        "    #    percent_out_dist = 12  # Third Shift: 12% OOD\n",
        "    #elif day <= 150:\n",
        "    #    percent_out_dist = 10 # Third month: 28% OOD\n",
        "    #else:\n",
        "    #    percent_out_dist = 5 # Fourth month: 7% OOD\n",
        "\n",
        "    daily_data = select_daily_data(\n",
        "        cosine_in_dist_similarities_VGG16_actual_PadChest,\n",
        "        cosine_out_dist_similarities_VGG16_actual_Pedatric,\n",
        "        percent_out_dist,\n",
        "        distances_per_day\n",
        "    )\n",
        "    all_data.append(daily_data)\n",
        "\n",
        "\n",
        "# Combine all daily data for CUSUM analysis\n",
        "avg_cusum_data = np.concatenate(all_data)\n",
        "print(len(avg_cusum_data))\n",
        "\n",
        "# This output can be used by 2-sigma and 3-sigma chart, as we are only using one value with them."
      ],
      "metadata": {
        "id": "dEMoGwV9zMIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The previous function uses single images for 2-sigma and 3-sigma charts\n",
        "# For CUSUM, we will use the average per day\n",
        "\n",
        "# Calculate the daily averages and the percentage of out-of-distribution data\n",
        "daily_averages = [np.mean(day) for day in all_data]\n",
        "percentages_out_dist = []\n",
        "\n",
        "for day in range(1, total_days + 1):\n",
        "      if day <= 30:\n",
        "        percent_out_dist = 2  # First month: 3% OOD\n",
        "      elif day <= 60:\n",
        "        percent_out_dist = 4  # Second month: 4% OOD\n",
        "      #elif day <= 70:\n",
        "      #  percent_out_dist = 7 # Third month: 7% OOD\n",
        "      #elif day <= 100:\n",
        "      #  percent_out_dist = 12  # Third month: 12% OOD\n",
        "      #elif day <= 150:\n",
        "      #  percent_out_dist = 10 # Third montn: 28% OOD\n",
        "      #else:\n",
        "      #  percent_out_dist = 5 # Fourth month: 7% OOD\n",
        "      percentages_out_dist.append(percent_out_dist)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(15, 6))\n",
        "sc = plt.scatter(range(1, total_days + 1), daily_averages, c=percentages_out_dist, cmap='coolwarm')\n",
        "plt.colorbar(sc, label='Percentage of Out-of-Distribution Data (%)')\n",
        "plt.title('Daily Average Cosine Distance with Out-of-Distribution Data Percentage')\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Average Cosine Distance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Calculate the daily averages\n",
        "daily_averages = [np.mean(day) for day in all_data]\n",
        "CUSUM_data_average_day = np.array(daily_averages)\n",
        "print(len(CUSUM_data_average_day))\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(range(1, total_days + 1), daily_averages, marker='o', linestyle='-', color='b')\n",
        "plt.title('Daily Average Cosine Distance Over Time')\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Average Cosine Distance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Now daily_averages can be used for further analysis like CUSUM\n",
        "#daily_averages[:60]  # Displaying the first 10 days as a sample\n",
        "#print(len(daily_averages))\n",
        "#print(np.mean(daily_averages[1:15]) )\n",
        "#print(np.mean(daily_averages[16:30]) )\n",
        "\n",
        "\n",
        "# Save the cosine similarity for each batch for each day.\n",
        "np.savetxt(\"/content/drive/My Drive/daily_averages_cosine_CUSUM_VGG16.csv\", daily_averages, delimiter=\",\", fmt='%s')\n"
      ],
      "metadata": {
        "id": "lFX5j6n5zw5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming the function `select_daily_data_with_labels` is already defined as earlier\n",
        "# Let's redefine it here for completeness\n",
        "def select_daily_data_with_labels(in_dist_data, out_dist_data, out_dist_percent, day_data_count):\n",
        "    out_dist_count = int(day_data_count * out_dist_percent / 100)\n",
        "    in_dist_count = day_data_count - out_dist_count\n",
        "\n",
        "    daily_out_dist_data = np.random.choice(out_dist_data, out_dist_count, replace=False)\n",
        "    daily_in_dist_data = np.random.choice(in_dist_data, in_dist_count, replace=False)\n",
        "\n",
        "    daily_out_dist_labels = ['out-dist'] * out_dist_count\n",
        "    daily_in_dist_labels = ['in-dist'] * in_dist_count\n",
        "\n",
        "    combined_data = np.concatenate((daily_in_dist_data, daily_out_dist_data))\n",
        "    combined_labels = np.concatenate((daily_in_dist_labels, daily_out_dist_labels))\n",
        "\n",
        "    combined = list(zip(combined_data, combined_labels))\n",
        "    np.random.shuffle(combined)\n",
        "\n",
        "    shuffled_data, shuffled_labels = zip(*combined)\n",
        "    return list(shuffled_data), list(shuffled_labels)\n",
        "\n",
        "# Assuming cosine_in_dist_similarities and cosine_out_dist_similarities are already defined\n",
        "# as arrays of cosine similarity values for in-distribution and out-of-distribution data\n",
        "\n",
        "# Now we select the data for the four days\n",
        "day_data = {}\n",
        "day_labels = {}\n",
        "# percentages_out_dist = [3, 4, 7, 12]  # Percentage for each period\n",
        "percentages_out_dist = [5, 12]  # Percentage for each period\n",
        "\n",
        "\n",
        "# Select one day before day 30\n",
        "day_data[1], day_labels[1] = select_daily_data_with_labels(\n",
        "    cosine_in_dist_similarities_VGG16_actual_PadChest,\n",
        "    cosine_out_dist_similarities_VGG16_actual_Pedatric,\n",
        "    percentages_out_dist[0],  # 3% out-dist\n",
        "    100\n",
        ")\n",
        "\n",
        "# Select one day between day 31 and day 60\n",
        "day_data[2], day_labels[2] = select_daily_data_with_labels(\n",
        "    cosine_in_dist_similarities_VGG16_actual_PadChest,\n",
        "    cosine_out_dist_similarities_VGG16_actual_Pedatric,\n",
        "    percentages_out_dist[1],  # 4% out-dist\n",
        "    100\n",
        ")\n",
        "\n",
        "# Select one day between day 41 and day 69\n",
        "# day_data[3], day_labels[3] = select_daily_data_with_labels(\n",
        "#    cosine_in_dist_similarities,\n",
        "#    cosine_out_dist_similarities,\n",
        "#    percentages_out_dist[2],  # 7% out-dist\n",
        "#    60\n",
        "#)\n",
        "\n",
        "# Select one day between day 71 and day 99\n",
        "#day_data[4], day_labels[4] = select_daily_data_with_labels(\n",
        "#    cosine_in_dist_similarities,\n",
        "#    cosine_out_dist_similarities,\n",
        "#    percentages_out_dist[3],  # 12% out-dist\n",
        "#    60\n",
        "#)\n",
        "\n",
        "# Now day_data contains the cosine similarities for the selected days\n",
        "# and day_labels contains the corresponding labels\n",
        "\n",
        "# We can now print the cosine similarities for each selected day\n",
        "for day in day_data:\n",
        "    print(f\"Day {day} Data:\")\n",
        "    print(day_data[day])\n",
        "    print(f\"Day {day} Labels:\")\n",
        "    print(day_labels[day])\n",
        "    print(\"\\n\")  # Print a newline for better readability between days\n",
        "\n",
        "#day_labels = {\n",
        "#    1: ['in-dist'] * 58 + ['out-dist'] * 2,\n",
        "#    2: ['in-dist'] * 57 + ['out-dist'] * 3,\n",
        "#    #3: ['in-dist'] * 56 + ['out-dist'] * 4,\n",
        "    #4: ['in-dist'] * 53 + ['out-dist'] * 7\n",
        "#}\n",
        "\n",
        "\n",
        "# Percentages of out-of-distribution data for each day\n",
        "percentages_out_dist = [5, 12]  # Extend this list as needed for more days\n",
        "\n",
        "day_labels = {}\n",
        "\n",
        "for day, out_dist_percentage in enumerate(percentages_out_dist, start=1):\n",
        "    total_data_count = 100  # Assuming each day has 100 data points\n",
        "    out_dist_count = int(total_data_count * out_dist_percentage / 100)\n",
        "    in_dist_count = total_data_count - out_dist_count\n",
        "\n",
        "    day_labels[day] = ['in-dist'] * in_dist_count + ['out-dist'] * out_dist_count\n",
        "\n",
        "# Now day_labels contains dynamically assigned labels based on percentages\n",
        "\n",
        "# Example: Print labels for each day\n",
        "for day, labels in day_labels.items():\n",
        "    print(f\"Day {day} Labels:\")\n",
        "    print(labels)\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Now we'll count the number of in-dist and out-dist points for each day.\n",
        "in_dist_count = {}\n",
        "out_dist_count = {}\n",
        "\n",
        "for day, labels in day_labels.items():\n",
        "    in_dist_count[day] = labels.count('in-dist')\n",
        "    out_dist_count[day] = labels.count('out-dist')\n",
        "\n",
        "in_dist_count, out_dist_count"
      ],
      "metadata": {
        "id": "AkhjPs-dz-er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " sample_days = [15, 35, 65, 95]\n",
        "sample_days = [18, 47]\n",
        "\n",
        "\n",
        "# Calculate the mean and standard deviation for in-distribution data\n",
        "mean_in_dist_day_bacth = np.mean(cosine_in_dist_similarities_VGG16_actual_PadChest)\n",
        "std_in_dist_day_batch = np.std(cosine_in_dist_similarities_VGG16_actual_PadChest)\n",
        "\n",
        "# Define the control limits using the three-sigma rule for in-distribution data\n",
        "upper_control_limit = mean_in_dist_day_bacth + 3 * std_in_dist_day_batch\n",
        "lower_control_limit = mean_in_dist_day_bacth - 3 * std_in_dist_day_batch\n",
        "\n",
        "\n",
        "# Define the function to select daily data with labels\n",
        "def select_daily_data_with_labels(in_dist_data, out_dist_data, out_dist_percent, day_data_count):\n",
        "    out_dist_count = int(day_data_count * out_dist_percent / 100)\n",
        "    in_dist_count = day_data_count - out_dist_count\n",
        "\n",
        "    daily_out_dist_data = np.random.choice(out_dist_data, out_dist_count, replace=False)\n",
        "    daily_in_dist_data = np.random.choice(in_dist_data, in_dist_count, replace=False)\n",
        "\n",
        "    daily_out_dist_labels = ['out-dist'] * out_dist_count\n",
        "    daily_in_dist_labels = ['in-dist'] * in_dist_count\n",
        "\n",
        "    combined_data = np.concatenate((daily_in_dist_data, daily_out_dist_data))\n",
        "    combined_labels = np.concatenate((daily_in_dist_labels, daily_out_dist_labels))\n",
        "\n",
        "    combined = list(zip(combined_data, combined_labels))\n",
        "    np.random.shuffle(combined)\n",
        "\n",
        "    shuffled_data, shuffled_labels = zip(*combined)\n",
        "    return list(shuffled_data), list(shuffled_labels)\n",
        "\n",
        "# Now we select the data for the four days with the specified out-of-distribution percentages\n",
        "# percentages_out_dist = [3, 4, 7, 12]  # Percentage for each period\n",
        "percentages_out_dist = [5,12]  # Percentage for each period\n",
        "day_data = {}\n",
        "day_labels = {}\n",
        "\n",
        "for i, percent_out_dist in enumerate(percentages_out_dist, start=1):\n",
        "    day_data[i], day_labels[i] = select_daily_data_with_labels(\n",
        "        cosine_in_dist_similarities_VGG16_actual_PadChest,\n",
        "        cosine_out_dist_similarities_VGG16_actual_Pedatric,\n",
        "        percent_out_dist,\n",
        "        100\n",
        "    )\n",
        "\n",
        "# Let's create a function to plot SPC chart for given day\n",
        "def plot_spc_chart(day_data, day_labels, upper_control_limit, lower_control_limit, day_number):\n",
        "    # Plot the SPC chart\n",
        "    fig, ax = plt.subplots(figsize=(15, 6))\n",
        "    ax.plot(day_data, marker='o', linestyle='-', color='k', label=f' {day_number} Data')  # Black color for plot\n",
        "\n",
        "    # Plot control limits and mean\n",
        "    ax.axhline(upper_control_limit, color='k', linestyle='--', label='Upper Control Limit (UCL)')  # Black color for UCL\n",
        "    ax.axhline(lower_control_limit, color='k', linestyle='--', label='Lower Control Limit (LCL)')  # Black color for LCL\n",
        "    ax.axhline(np.mean(daily_averages[1:20]), color='k', linestyle='-', label='Mean')  # Black color for mean\n",
        "\n",
        "\n",
        "    # Highlight out-of-distribution points\n",
        "    for i, (val, label) in enumerate(zip(day_data, day_labels)):\n",
        "        if label == 'out-dist':\n",
        "            ax.scatter(i, val, color='darkgrey', marker='*', s=150)  # Red circles for out-of-control points\n",
        "    # Highlight out-of-distribution points and draw a circle around them as known ground truth\n",
        "    for i, (val, label) in enumerate(zip(day_data, day_labels)):\n",
        "        if label == 'out-dist':\n",
        "            #ax.scatter(i, val, color='grey', marker='o', s=250)  # Red circles for known out-of-control points\n",
        "            ax.scatter(i, val, facecolors='none', edgecolors='grey', marker='o', s=250)  # Circle around the point\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ax.set_facecolor('white')  # White background\n",
        "    ax.set_title(f'SPC Chart for {day_number}')\n",
        "    ax.set_xlabel('Image Sequence')\n",
        "    ax.set_ylabel('Cosine Similarity')\n",
        "    ax.legend()\n",
        "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)  # Lighter grid lines for better visibility\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Now we plot the SPC charts with \"Day 15\", \"Day 35\", \"Day 65\", and \"Day 95\" as titles\n",
        "for day, day_number in enumerate(sample_days, start=1):\n",
        "  day_title = f'Day {day_number}'  # Create a title string for the chart\n",
        "  plot_spc_chart(day_data[day], day_labels[day], upper_control_limit, lower_control_limit, day_title)\n"
      ],
      "metadata": {
        "id": "hMnrBGt10XoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Three sigma chart for the average vaue\n",
        "# Define the control limits using the three-sigma rule\n",
        "upper_control_limit = np.mean(daily_averages[:30]) + 3 * np.std(daily_averages[:30])\n",
        "lower_control_limit = np.mean(daily_averages[:30]) - 3 * np.std(daily_averages[:30])\n",
        "\n",
        "print(np.mean(daily_averages[1:30]))\n",
        "print(np.mean(daily_averages[31:60]))\n",
        "\n",
        "shift_start_day = 31\n",
        "#print(upper_control_limit)\n",
        "#print(lower_control_limit)\n",
        "\n",
        "# Create Bernoulli CUSUM vectors\n",
        "average_binary_3sigma_vector = create_control_vector(daily_averages, upper_control_limit, lower_control_limit)\n",
        "average_ground_truth_vector = create_shift_vector(len(daily_averages), shift_start_day)\n",
        "\n",
        "\n",
        "# Plot the SPC chart\n",
        "fig, ax = plt.subplots(figsize=(15, 6))\n",
        "ax.plot(daily_averages, marker='o', linestyle='-', color='k', label='Daily Averages')  # Black color for plot\n",
        "\n",
        "# Plot control limits and mean\n",
        "ax.axhline(upper_control_limit, color='k', linestyle='--', label='Upper Control Limit (UCL)')  # Black color for UCL\n",
        "ax.axhline(lower_control_limit, color='k', linestyle='--', label='Lower Control Limit (LCL)')  # Black color for LCL\n",
        "ax.axhline(np.mean(daily_averages[1:30]), color='k', linestyle='-', label='Mean')  # Black color for mean\n",
        "\n",
        "# Highlight points outside of control limits with grey circle around them\n",
        "for i, val in enumerate(daily_averages):\n",
        "    if val > upper_control_limit or val < lower_control_limit:\n",
        "        ax.scatter(i, val, color='darkgrey', marker='*', s=150)  # Dark grey stars for out-of-control points\n",
        "        #ax.scatter(i, val, facecolors='none', edgecolors='grey', marker='o', s=250)  # Grey circle around out-of-control points\n",
        "\n",
        "# Circle all points after the shift point\n",
        "for i in range(30, len(daily_averages)):\n",
        "    ax.scatter(i, daily_averages[i], facecolors='none', edgecolors='grey', marker='o', s=250)  # Grey circle around points after shift\n",
        "\n",
        "\n",
        "\n",
        "# Indicate the first shift point\n",
        "ax.axvline(x=30, color='purple', linestyle='--', label='Low Shift (2-4%)')  # Purple line for shift start\n",
        "# Indicate the second shift point\n",
        "#ax.axvline(x=40, color='purple', linestyle='--', label='Second Shift (moderate)')  # Purple line for shift start\n",
        "# Indicate the third shift point\n",
        "#ax.axvline(x=70, color='purple', linestyle='--', label='Third Shift (high)')  # Purple line for shift start\n",
        "\n",
        "ax.set_facecolor('white')  # White background\n",
        "ax.set_xlabel('Time (Day)')\n",
        "ax.set_ylabel('Average Cosine Similarity ')\n",
        "ax.legend()\n",
        "ax.grid(True, which='both', linestyle='--', linewidth=0.5)  # Lighter grid lines for better visibility\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Nx93dZkb06VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,precision_score,recall_score,classification_report, f1_score\n",
        "\n",
        "conf_matrix = confusion_matrix(average_ground_truth_vector, average_binary_3sigma_vector)\n",
        "\n",
        "# Calculate True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN)\n",
        "TP, FP, FN, TN = conf_matrix.ravel()\n",
        "\n",
        "# Calculate accuracy, sensitivity (recall), and specificity\n",
        "accuracy = accuracy_score(average_ground_truth_vector, average_binary_3sigma_vector)\n",
        "sensitivity = TP / (TP + FN)  # Recall\n",
        "specificity = TN / (TN + FP)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "print(\"\\nAccuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"Sensitivity (Recall): {:.2f}%\".format(sensitivity * 100))\n",
        "print(\"Specificity: {:.2f}%\".format(specificity * 100))\n",
        "\n"
      ],
      "metadata": {
        "id": "tqEi5tSu1D9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CUSUM function\n",
        "def CUSUM(x, mu0, k, h):\n",
        "    S_hi = [0]\n",
        "    S_lo = [0]\n",
        "    for i in range(len(x)):\n",
        "        S_hi.append(max(0, S_hi[i] + (x[i] - mu0 - k)))\n",
        "        S_lo.append(min(0, S_lo[i] + (x[i] - mu0 + k)))\n",
        "\n",
        "    S_hi = np.array(S_hi[1:])\n",
        "    S_lo = np.array(S_lo[1:])\n",
        "\n",
        "    signal_hi = np.where(S_hi > h)[0]\n",
        "    signal_lo = np.where(S_lo < -h)[0]\n",
        "    signal = np.unique(np.concatenate((signal_hi, signal_lo)))\n",
        "\n",
        "    return signal, S_hi, S_lo"
      ],
      "metadata": {
        "id": "GXE-K_E71aYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of k values as a fraction of in_std\n",
        "k_values = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
        "# Specify the control parameters and the threshold\n",
        "pre_change_days = 30  # Number of days the process is in-control\n",
        "total_days = 60  # Total number of days in the dataset\n",
        "control_limit = 4  # Multiplier for control limit\n",
        "delta = 1  # Change magnitude in terms of standard deviations\n",
        "\n",
        "# Split your data into in-control and out-of-control periods\n",
        "in_control_sp = CUSUM_data_average_day[:pre_change_days]\n",
        "out_control_sp = CUSUM_data_average_day[pre_change_days:total_days]\n",
        "\n",
        "# Compute the mean and standard deviation for in-control and out-of-control periods\n",
        "mu_in = np.mean(in_control_sp)\n",
        "mu_out = np.mean(out_control_sp)\n",
        "in_std = np.std(in_control_sp)\n"
      ],
      "metadata": {
        "id": "gM7ncQ7c3VsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df = pd.DataFrame(columns=[\"Threshold\", \"False Positives\", \"True Positives\", \"Average Detection Delay\", \"MTBFA\", \"False Alarm Rate\"])\n",
        "\n",
        "for k_th in k_values:\n",
        "    k = k_th * in_std\n",
        "    h = control_limit * in_std  # h equal to 4*std\n",
        "\n",
        "    # Call the CUSUM function\n",
        "    signal, S_hi, S_lo = CUSUM(CUSUM_data_average_day, mu_in, k, h)\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(15, 6))\n",
        "\n",
        "    ax.plot(S_hi, label='High Side CUSUM', color='blue')\n",
        "    ax.plot(S_lo, label='Low Side CUSUM', color='green')\n",
        "    ax.axhline(y=h, color='black', linestyle='--', linewidth=2, label='Threshold (+h)')\n",
        "    ax.axhline(y=-h, color='black', linestyle='--', linewidth=2, label='Threshold (-h)')\n",
        "    ax.scatter(signal, [S_hi[i] for i in signal], color='black', zorder=5, label='Detected Shift')\n",
        "    ax.scatter(signal, [S_lo[i] for i in signal], color='black', zorder=5)\n",
        "\n",
        "    # Adding vertical lines for expected shift points (every 30 days starting from day 30)\n",
        "    for day in range(30, total_days+1, 60):\n",
        "      ax.axvline(x=day, color='purple', linestyle='--', label='Expected Shift Point' if day == 30 else \"\")\n",
        "\n",
        "    #ax.set_title(f'Processing for k = {k}')\n",
        "    ax.set_facecolor('white')  # White background\n",
        "\n",
        "    ax.set_xlabel('Image Sequence')\n",
        "    ax.set_ylabel('CUSUM Value')\n",
        "    ax.legend()\n",
        "    ax.grid(True, color='lightgrey')  # Black grid lines\n",
        "    plt.show()\n",
        "\n",
        "     # Calculate False Positives\n",
        "    for i in range(pre_change_days):\n",
        "        if S_hi[i] > h or S_lo[i] > -h:  # Assuming symmetry around zero for S_lo\n",
        "            FalsePos.append(i + 1)\n",
        "            DetectionTimes.append(i + 1)\n",
        "\n",
        "    # Calculate True Positives and Detection Delay\n",
        "    for i in range(pre_change_days, total_days):\n",
        "        if S_hi[i] > h or S_lo[i] > -h:\n",
        "            TruePos.append(i + 1)\n",
        "            AvgDD.append(i + 1 - pre_change_days)\n",
        "            break  # Remove this break if you want to count all true positives\n"
      ],
      "metadata": {
        "id": "V9qOicxO1qdN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}